@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2023context,
  title={In-context learning with many demonstration examples},
  author={Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2302.04931},
  year={2023}
}

@article{Huang_Cao_Parulian_Ji_Wang_2021,   title={Efficient Attentions for Long Document Summarization},  journal={Cornell University - arXiv,Cornell University - arXiv},  author={Huang, Luyang and Cao, Shijie and Parulian, NikolausNova and Ji, Heng and Wang, Lu},  year={2021},  month={Apr},  language={en-US}  }

@misc{yang2018hotpotqa,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bloc97,
    title = {{NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation}},
    author = {bloc97},
    url = "https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/",
    year = 2023
}

@article{DBLP:journals/corr/abs-2106-09685,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bloc972,
    title = {{Add NTK-Aware interpolation "by parts" correction}},
    author = {bloc97},
    url = "https://github.com/jquesnelle/scaled-rope/pull/1",
    year = 2023
}

@misc{javaheripi2023phi,
    title={Phi-2: The surprising power of small language models},
    author={Mojan Javaheripi and Sébastien Bubeck and Marah Abdin and Jyoti Aneja and Sebastien Bubeck and Caio César Teodoro Mendes and Weizhu Chen and Allie Del Giorno and Ronen Eldan and Sivakanth Gopi and Suriya Gunasekar and Mojan Javaheripi and Piero Kauffmann and Yin Tat Lee and Yuanzhi Li and Anh Nguyen and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Michael Santacroce and Harkirat Singh Behl and Adam Taumann Kalai and Xin Wang and Rachel Ward and Philipp Witte and Cyril Zhang and Yi Zhang},
    year={2023},
}

@misc{redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce Llama Training Dataset},
  year = {2023},
  howpublished = {\url{https://github.com/togethercomputer/RedPajama-Data}},
}

@misc{chen2023longlora,
      title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models}, 
      author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
      year={2023},
      eprint={2309.12307},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mohtashami2023landmark,
      title={Landmark Attention: Random-Access Infinite Context Length for Transformers}, 
      author={Amirkeivan Mohtashami and Martin Jaggi},
      year={2023},
      eprint={2305.16300},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{han2023lminfinite,
      title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models}, 
      author={Chi Han and Qifan Wang and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
      year={2023},
      eprint={2308.16137},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2022lengthextrapolatable,
      title={A Length-Extrapolatable Transformer}, 
      author={Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},
      year={2022},
      eprint={2212.10554},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jin2024llm,
      title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, 
      author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
      year={2024},
      eprint={2401.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@misc{peng2023yarn,
      title={YaRN: Efficient Context Window Extension of Large Language Models}, 
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023extending,
      title={Extending Context Window of Large Language Models via Positional Interpolation}, 
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      eprint={2306.15595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023longllmlingua,
      title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression}, 
      author={Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2023},
      eprint={2310.06839},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{izacard2022unsupervised,
      title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
      year={2022},
      eprint={2112.09118},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{bai2023longbench,
      title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding}, 
      author={Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2023},
      eprint={2308.14508},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{an2023leval,
      title={L-Eval: Instituting Standardized Evaluation for Long Context Language Models}, 
      author={Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu},
      year={2023},
      eprint={2307.11088},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dong2023bamboo,
      title={BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models}, 
      author={Zican Dong and Tianyi Tang and Junyi Li and Wayne Xin Zhao and Ji-Rong Wen},
      year={2023},
      eprint={2309.13345},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rae2019compressive,
      title={Compressive Transformers for Long-Range Sequence Modelling}, 
      author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Timothy P. Lillicrap},
      year={2019},
      eprint={1911.05507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{azerbayev2023proofnet,
      title={ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics}, 
      author={Zhangir Azerbayev and Bartosz Piotrowski and Hailey Schoelkopf and Edward W. Ayers and Dragomir Radev and Jeremy Avigad},
      year={2023},
      eprint={2302.12433},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Su_Lu_Pan_Wen_Liu_2021,   title={RoFormer: Enhanced Transformer with Rotary Position Embedding},  journal={Cornell University - arXiv,Cornell University - arXiv},  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},  year={2021},  month={Apr},  language={en-US}  }

@misc{emozillareddit,
    title = {{Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning}},
    author = {emozilla},
    url = "https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/",
    year = 2023
}

@misc{sun2023retentive,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tworkowski2023focused,
      title={Focused Transformer: Contrastive Training for Context Scaling}, 
      author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
      year={2023},
      eprint={2307.03170},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2024retrieval,
      title={Retrieval meets Long Context Large Language Models}, 
      author={Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2310.03025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{li2023compressing,
      title={Compressing Context to Enhance Inference Efficiency of Large Language Models}, 
      author={Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin},
      year={2023},
      eprint={2310.06201},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023loogle,
      title={LooGLE: Can Long-Context Language Models Understand Long Contexts?}, 
      author={Jiaqi Li and Mengmeng Wang and Zilong Zheng and Muhan Zhang},
      year={2023},
      eprint={2311.04939},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tay2020long,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{NeedleInAHaystack,
  author       = {gkamradt},
  title        = {Needle In A Haystack - Pressure Testing LLMs},
  url        = {https://github.com/gkamradt/LLMTest_NeedleInAHaystack},
  year         = {2023},
}

@misc{sun2021longrange,
      title={Do Long-Range Language Models Actually Use Long-Range Context?}, 
      author={Simeng Sun and Kalpesh Krishna and Andrew Mattarella-Micke and Mohit Iyyer},
      year={2021},
      eprint={2109.09115},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xiao2024infllm,
  title={InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.04617},
  year={2024}
}

@article{lu2024longheads,
  title={LongHeads: Multi-Head Attention is Secretly a Long Context Processor},
  author={Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.10685},
  year={2024}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{fu2024data,
      title={Data Engineering for Scaling Language Models to 128K Context}, 
      author={Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hannaneh Hajishirzi and Yoon Kim and Hao Peng},
      year={2024},
      eprint={2402.10171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@misc{xiao2023efficient,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2023},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{huang-etal-2021-efficient,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.112",
    doi = "10.18653/v1/2021.naacl-main.112",
    pages = "1419--1436",
    abstract = "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
}

@article{Kočiský_Schwarz_Blunsom_Dyer_Hermann_Melis_Grefenstette_2018,   title={The NarrativeQA Reading Comprehension Challenge},  url={http://dx.doi.org/10.1162/tacl_a_00023},  DOI={10.1162/tacl_a_00023},  journal={Transactions of the Association for Computational Linguistics},  author={Kočiský, Tomáš and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, Gábor and Grefenstette, Edward},  year={2018},  month={Dec},  pages={317–328},  language={en-US}  }

@misc{an2024trainingfree,
      title={Training-Free Long-Context Scaling of Large Language Models}, 
      author={Chenxin An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
      year={2024},
      eprint={2402.17463},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2024efficient,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hsieh2024ruler,
      title={RULER: What's the Real Context Size of Your Long-Context Language Models?}, 
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023lost,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2024soaring,
      title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon}, 
      author={Peitian Zhang and Zheng Liu and Shitao Xiao and Ninglu Shao and Qiwei Ye and Zhicheng Dou},
      year={2024},
      eprint={2401.03462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{rozière2024code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{su2023roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{together-instruct,
  author       = {Together.AI},
  title        = {Llama-2-7B-32K-Instruct — and fine-tuning for Llama-2 models with Together API},
  year         = {2023},
  url          = {https://www.together.ai/blog/llama-2-7b-32k-instruct},
}



@misc{zhang2024extending,
      title={Extending LLMs' Context Window with 100 Samples}, 
      author={Yikai Zhang and Junlong Li and Pengfei Liu},
      year={2024},
      eprint={2401.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lin2023train,
      title={How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval}, 
      author={Sheng-Chieh Lin and Akari Asai and Minghan Li and Barlas Oguz and Jimmy Lin and Yashar Mehdad and Wen-tau Yih and Xilun Chen},
      year={2023},
      eprint={2302.07452},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{child2019generating,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ho2019axial,
      title={Axial Attention in Multidimensional Transformers}, 
      author={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},
      year={2019},
      eprint={1912.12180},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zaheer2021big,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhu2021longshort,
      title={Long-Short Transformer: Efficient Transformers for Language and Vision}, 
      author={Chen Zhu and Wei Ping and Chaowei Xiao and Mohammad Shoeybi and Tom Goldstein and Anima Anandkumar and Bryan Catanzaro},
      year={2021},
      eprint={2107.02192},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hu2024perplexity,
    title={Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?},
    author={Yutong Hu and Quzhe Huang and Mingxu Tao and Chen Zhang and Yansong Feng},
    year={2024},
    eprint={2405.06105},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
tanzer2024a,
title={A Benchmark for Learning to Translate a New Language from One Grammar Book},
author={Garrett Tanzer and Mirac Suzgun and Eline Visser and Dan Jurafsky and Luke Melas-Kyriazi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tbVWug9f2h}
}

@article{bertsch2024context,
  title={In-Context Learning with Long-Context Models: An In-Depth Exploration},
  author={Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
  journal={arXiv preprint arXiv:2405.00200},
  year={2024}
}

@inproceedings{kryscinski2022booksum,
  title={BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization},
  author={Kry{\'s}ci{\'n}ski, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={6536--6558},
  year={2022}
}

@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@inproceedings{
liu2023ring,
title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
author={Hao Liu and Matei Zaharia and Pieter Abbeel},
booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
year={2023},
url={https://openreview.net/forum?id=fXugVDtCQO}
}

@misc{kontonis2024active,
      title={Active Learning with Simple Questions}, 
      author={Vasilis Kontonis and Mingchen Ma and Christos Tzamos},
      year={2024},
      eprint={2405.07937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@misc{cerebras2023slimpajama,
author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
month = June,
year = 2023,
howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}

@misc{chen2024clexcontinuouslengthextrapolation,
      title={CLEX: Continuous Length Extrapolation for Large Language Models}, 
      author={Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing},
      year={2024},
      eprint={2310.16450},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16450}, 
}

@inproceedings{Li_Roth_2002,  
 title={Learning question classifiers}, 
 url={http://dx.doi.org/10.3115/1072228.1072378}, 
 DOI={10.3115/1072228.1072378}, 
 booktitle={Proceedings of the 19th international conference on Computational linguistics  -}, 
 author={Li, Xin and Roth, Dan}, 
 year={2002}, 
 month={Jan}, 
 language={en-US} 
}
